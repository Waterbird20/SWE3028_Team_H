\subsection{Problem Statement}

\subsubsection{Determining Data Set}

Data collection will be conducted using the FinanceDataReader package in Python (\url{https://github.com/FinanceData/FinanceDataReader}). 
This package serves as a data crawler, extracting information from \url{https://data.krx.co.kr} for Korean stock data and \url{http://old.nasdaq.com} for US stock data. 
It provides access to historical and up-to-date data, making it suitable for both prediction and training/validation/testing purposes.

\subsubsection{Determining Model}

Stock prices exhibit one-dimensional, non-stationary, time-series characteristics. 
In consideration of these unique traits, the selection of an appropriate model is crucial.

\subsubsection{Determining Loss Function and Evaluation Metric}

The choice of a loss function and evaluation metric hinges on the nature of our stock price prediction task. 
If we are predicting only daily price movements (rise/fall), the appropriate loss function will be Cross Entropy, and the evaluation metric will be Accuracy. 
Conversely, if we aim to predict specific values such as closing prices, the preferred loss function will be Huber Loss or Mean Squared Error (MSE), 
and the evaluation metric will be Mean Absolute Error (MAE) or Root Mean Square Error (RMSE).


% (Loss Function과 평가 지표는 저희가 어떤 방식으로 예측할 것인가에 따라 달렸는데, 
% 일일 상승/하락만 예측한다면 Loss Function은 Cross Entropy, 평가 지표는 Accuracy로 갈 것이고, 
% 값(종가)을 예측한다면 Loss Function은 Huber Loss혹은 MSE로 갈 것이고, 평가 지표는 Mean Absolute Error 혹은 Root Mean Squared Error로 가면 될 것 같습니다. 예측 방식도 논의를 해 봐야 할 것 같네요. Proposal 발표 때에는 일일 상승/하락을 예측할 예정이라고 발표했던 기억이 희미하게 납니다.)

% Loss Function 중 생소하실만한 Huber Loss에 대해 간략하게 설명하자면, 
% Huber Loss는 Mean Squared Error와 Mean Absolute Error를 합친 Loss Function입니다. 
% 오차를 절댓값으로 계산한다면 0인 지점에서 미분이 불가한 문제가 발생하고, 
% Mean Squared Error를 사용하자니 Loss가 순식간에 커진다는 단점이 있습니다. 
% Huber Loss는 0 부근에서는 MSE, 그 밖에는 MAE를 사용하여 MSE와 MAE가 만나는 지점은 조정을 통해 미분 가능하도록 계수를 조정한 Loss Function입니다. 
% 두 마리 토끼를 다 잡는 셈이죵

% \subsubsection{Deploying the Model}

% 모델 선정 후, AWS 서버 구축을 통해 서비스를 배포한다

% 라는 식으로 Problem Statement는 작성하면 좋을 것 같습니다!




\subsection{Proposed Solutions}
\subsubsection{LSTM}
Long Short-Term Memory(LSTM) is an advanced Recurrent Neural Network (RNN) architecture 
as shown in the Figure. 
LSTMs were introduced to address some of the limitations of traditional RNNs, 
which struggle with capturing long-range dependencies in sequential data due to the vanishing gradient problem.

LSTM has three gates: input gate, forget gate, and output gate.
\begin{enumerate}
	\item Input gate: 	Input gate is denoted by orange box. It decides what new information should be stored in the cell.
	\item Forget gate:	Forget gate is denoted by blue box. It determines what information from the previous state should be discarded or reflected.
	\item Output gate:	Output gate is denoted by gray box. The actual outputs are $h_{i}$ and $y_{i}$, which are same and $c_{i}$ represents the status of the cell. It specifies what information from the cell should be used to generate the output.
\end{enumerate}

Compared to the traditional RNN, LSTM performs various mathematical operations, including including element-wise multiplication and addition, to control the flow of information and perform updates to the memory cell and hidden state.

Through this architecture and characteristics, LSTM can handle the long sequential data by maintaining a memory cell with gates to control information flow, 
making it capable of capturing long-term dependencies and patterns in the data.

\subsubsection{GRU}

A Gated Recurrent Unit(GRU) is another type of recurrent neural network (RNN) architecture, 
similar to the Long Short-Term Memory (LSTM) network. 
GRUs are simpler in structure compared to LSTMs but have been found to be highly effective in various applications. 
GRU is also designed to address the vanishing gradient problem and enable RNNs to better capture long-range dependencies in sequential data. 
Compared to LSTM, GRU does not distinguish between cell status and the output.

% Input gate, forget gate?
GRU has two gates: reset gate and update gate.
\begin{enumerate}
	\item Reset gate: Reset gate is denoted by color box. It decides how much of the past information to forget.
	\item Update gate: Update gate is denoted by color box. It decides how much of the past information to remember.
\end{enumerate}

GRUs also perform mathematical operations, including element-wise multiplications and additions, to control the flow of information and update the hidden state.

Through this architecture and characteristics, GRU can also handle the long sequential data by maintaining a memory cell with gates to control information flow, 
making it capable of capturing long-term dependencies and patterns in the data.


\subsubsection{One-dimensional CNN}

A Convolutional Neural Network(CNN) is a neural network architecture widely employed for processing and analyzing one-dimensional data sequences. 
In the context of stock price prediction, which inherently involves one-dimensional data, the utilization of a one-dimensional CNN is particularly relevant and effective.

Compared to other recurrent neural network (RNN) variants like LSTM and GRU, CNNs offer a notably simpler structural design. 
Also it seems possible to analyse the various patterns of stock price data through the convolutional layers.
As the stock price data is characterized by its non-stationary nature, exhibiting evolving trends and patterns over time, 
it is probable that CNN excels the performance of LSTM, and GRU.

\subsubsection{Transformer}

Transformers, known for their effectiveness in natural language processing, have also shown promise in sequence-to-sequence tasks. 
We can explore transformer-based models for capturing complex relationships in stock price data.
